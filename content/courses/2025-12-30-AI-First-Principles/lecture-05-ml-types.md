---
title: "Lecture 05 â€” Types of Machine Learning"
date: "2025-12-30"
type: book
weight: 50
---

<!--more-->

{{< icon name="clock" pack="fas" >}} ~3 hours (core AI taxonomy lecture)

---

## ğŸ§  Big Picture (Read This First)

> **Machine Learning = systems that improve performance by learning from experience.**

But here is the secret:

> Not all experience teaches the same way.

Different problems â†’ different learning paradigms.

If you choose the *wrong type*, even the best model will fail.

---

## ğŸ§­ A Simple Mental Map

Think of learning like school:

| Learning Type | Analogy |
|--------------|--------|
| Supervised | Teacher gives answers |
| Unsupervised | Student explores alone |
| Semi-supervised | Few answers, many questions |
| Self-supervised | Student creates own homework |
| Reinforcement | Learning by reward & punishment |

---

# 1ï¸âƒ£ Supervised Learning  
**â€œLearning with a teacherâ€**

---

## ğŸ“˜ Definition

Learning from **labeled data**:

```

(input â†’ correct output)

````

Examples:
- image â†’ label (cat / dog)
- email â†’ spam / not spam
- house features â†’ price

---

## ğŸ§  Intuition (Human Version)

A teacher says:
> â€œThis is a cat.â€  
> â€œThis is NOT a cat.â€

The student adjusts until mistakes are small.

---

## ğŸ§ª Classic Problems

### ğŸ”¹ Classification
- spam detection
- medical diagnosis
- fraud detection

### ğŸ”¹ Regression
- house price prediction
- temperature forecasting

---

## ğŸ’» Mini Project (Beginner-Friendly)

### ğŸ¯ Project: Email Spam Classifier

**Steps:**
1. Collect labeled emails
2. Convert text â†’ numbers
3. Train a classifier
4. Evaluate accuracy

```python
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import CountVectorizer

texts = ["free money", "meeting tomorrow"]
labels = [1, 0]  # 1=spam, 0=not spam

vec = CountVectorizer()
X = vec.fit_transform(texts)

model = LogisticRegression()
model.fit(X, labels)
````

ğŸ‘‰ This is **real AI**, not magic.

---

## â“ Quiz

{{< spoiler text="Why does supervised learning need labels?" >}}
Because the loss function compares predictions with ground truth.
{{< /spoiler >}}

---

# 2ï¸âƒ£ Unsupervised Learning

**â€œLearning without answersâ€**

---

## ğŸ“˜ Definition

Learning **patterns without labels**.

The system asks:

> â€œWhat structure exists here?â€

---

## ğŸ˜„ Funny Example

You walk into a party with strangers.

No name tags.

Your brain automatically groups people:

* similar clothes
* similar behavior
* similar interests

Thatâ€™s **clustering**.

---

## ğŸ§ª Common Techniques

* K-means clustering
* PCA (dimensionality reduction)
* Topic modeling

---

## ğŸ’» Mini Project

### ğŸ¯ Project: Customer Segmentation

```python
from sklearn.cluster import KMeans

X = [[20, 500], [25, 700], [45, 2000]]  # age, spending
kmeans = KMeans(n_clusters=2)
kmeans.fit(X)

print(kmeans.labels_)
```

Used in:

* marketing
* recommendation systems
* anomaly detection

---

## â“ Quiz

{{< spoiler text="Is unsupervised learning useful without labels?" >}}
Yes â€” it discovers structure and representations.
{{< /spoiler >}}

---

# 3ï¸âƒ£ Semi-Supervised Learning

**â€œA few answers, many questionsâ€**

---

## ğŸ“˜ Definition

Small labeled dataset
+
Large unlabeled dataset

---

## ğŸ§  Why This Exists

Labeling costs:

* money ğŸ’°
* time â³
* experts ğŸ§ 

But raw data is everywhere.

---

## ğŸ§ª Real-World Uses

* medical images
* legal documents
* speech recognition

---

## ğŸ’» Mini Project Idea

Label:

* 100 images by hand

Let the model:

* infer the rest

This is how **real-world AI systems survive**.

---

## â“ Quiz

{{< spoiler text="Why is semi-supervised learning practical?" >}}
Labels are expensive; data is cheap.
{{< /spoiler >}}

---

# 4ï¸âƒ£ Self-Supervised Learning

**â€œThe AI creates its own teacherâ€**

---

## ğŸ“˜ Definition

Labels are **automatically generated from data itself**.

---

## ğŸ¤¯ Why This Changed AI Forever

Instead of humans saying:

> â€œThis is the answerâ€

The system asks:

> â€œWhat part of the data is missing?â€

---

## ğŸ§ª Famous Examples

### ğŸ”¹ NLP

* Masked word prediction (BERT)

### ğŸ”¹ Vision

* Predict image rotation
* Contrastive learning (CLIP)

---

## ğŸ’» Mini Project (Conceptual)

```text
Input: "AI is ____"
Target: "powerful"
```

This simple idea trained **LLMs**.

---

## â“ Quiz

{{< spoiler text="Why is self-supervised learning powerful?" >}}
It scales with data, not human labeling.
{{< /spoiler >}}

---

# 5ï¸âƒ£ Reinforcement Learning (RL)

**â€œLearning by doingâ€**

---

## ğŸ“˜ Definition

Learning from **reward signals**, not correct answers.

---

## ğŸ•¹ï¸ Intuition

You train a dog ğŸ•:

* Good behavior â†’ treat
* Bad behavior â†’ no treat

No explicit instructions.

---

## ğŸ§ª Key Components

| Term        | Meaning  |
| ----------- | -------- |
| Agent       | Learner  |
| Environment | World    |
| Action      | Choice   |
| Reward      | Feedback |

---

## ğŸ’» Tiny RL Example (Concept)

```python
if reward > 0:
    increase_probability(action)
else:
    decrease_probability(action)
```

Used in:

* AlphaGo
* robotics
* recommendation systems
* game AI

---

## â“ Quiz

{{< spoiler text="What is the explorationâ€“exploitation tradeoff?" >}}
Balancing trying new actions vs using known good ones.
{{< /spoiler >}}

---

## ğŸ§  How These Power Modern AI

| System       | Learning Type                |
| ------------ | ---------------------------- |
| ChatGPT      | Self-supervised + RLHF       |
| Image AI     | Self-supervised + supervised |
| Robots       | Reinforcement learning       |
| Recommenders | Supervised + RL              |

---

## ğŸ“ Final Big Insight

> There is no â€œbestâ€ learning type.
> There is only **the right tool for the problem**.

Great AI engineers choose wisely.

---

## ğŸŒ± Final Reflection

{{< spoiler text="If you had unlimited data but no labels, which learning paradigm would you choose?" >}}
Self-supervised learning.
{{< /spoiler >}}