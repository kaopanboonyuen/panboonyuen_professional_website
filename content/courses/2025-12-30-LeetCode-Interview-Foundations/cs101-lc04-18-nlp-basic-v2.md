---
title: LC-TEST-18 â€” Basic NLP (V2)
weight: 120
date: '2026-01-01'
type: book
---

## ğŸ“˜ What Is TF-IDF? (Intuition + Math)

TF-IDF stands for:

> **Term Frequency â€“ Inverse Document Frequency**

It answers **one fundamental NLP question**:

> â Which words are important in this document,  
> compared to the entire collection? â

---

## ğŸ§® The Math (Step by Step)

### 1ï¸âƒ£ Term Frequency (TF)

How often a word appears **inside one document**.

$$
\text{TF}(w, d) =
\frac{\text{count of } w \text{ in document } d}
{\text{total words in document } d}
$$

ğŸ‘‰ Measures **local importance**

---

### 2ï¸âƒ£ Document Frequency (DF)

How many documents contain the word.

$$
\text{DF}(w) = \text{number of documents containing } w
$$

ğŸ‘‰ Measures **how common a word is across documents**

---

### 3ï¸âƒ£ Inverse Document Frequency (IDF)

Penalizes words appearing in many documents.

$$
\text{IDF}(w) = \log\left(\frac{N}{\text{DF}(w)}\right)
$$

Where:

- $N$ = total number of documents

ğŸ‘‰ Measures **global rarity**

---

### 4ï¸âƒ£ TF-IDF (Final Score)

$$
\text{TF-IDF}(w, d) =
\text{TF}(w, d) \times \text{IDF}(w)
$$

This is the **importance score** used in:
- search engines
- document ranking
- classical NLP systems

---

## ğŸ§ª Worked Example (Very Important)

### Corpus (3 documents)

```text
D1: "messi scores goals"
D2: "goals win matches"
D3: "messi inspires fans"
````

---

### Step 1: Vocabulary

```text
{messi, scores, goals, win, matches, inspires, fans}
```

---

### Step 2: Document Frequency (DF)

| Word     | DF |
| -------- | -- |
| messi    | 2  |
| goals    | 2  |
| scores   | 1  |
| win      | 1  |
| matches  | 1  |
| inspires | 1  |
| fans     | 1  |

---

### Step 3: IDF (Assume natural log)

$$
\text{IDF}(\text{messi}) = \log\left(\frac{3}{2}\right)
$$

$$
\text{IDF}(\text{scores}) = \log\left(\frac{3}{1}\right)
$$

ğŸ‘‰ `"scores"` appears in fewer documents
ğŸ‘‰ therefore it has **higher IDF**

---

### Step 4: TF-IDF in Document 1

Document 1 contains **3 words**:

```text
messi scores goals
```

| Word   | TF  | IDF      | TF-IDF |
| ------ | --- | -------- | ------ |
| messi  | 1/3 | log(3/2) | low    |
| scores | 1/3 | log(3/1) | HIGH   |
| goals  | 1/3 | log(3/2) | low    |

âœ… **"scores" becomes the most important word**

---

## ğŸ¯ What Interviewers Want You To Say

If asked:

> â€œExplain TF-IDFâ€

A **perfect answer** is:

> â€œTF-IDF measures how important a word is to a document
> relative to the whole corpus.
>
> It boosts words that are frequent in one document
> but rare across documents.â€

---

## ğŸ”— Why TF-IDF Still Matters Today

Even in the age of GPT:

* Search engines still use it
* Keyword extraction uses it
* It explains **why attention exists**
* It is the **conceptual ancestor of embeddings**

If you understand TF-IDF deeply,
**transformers feel natural â€” not magical**.

---

## ğŸŒ Why This Document Exists

Large Language Models do **not begin with transformers**.

They begin with a simple question:

> â€œWhich words matter?â€

TF-IDF is the **bridge** between:
- raw text
- statistics
- semantic meaning

Every NLP engineer **must master this**.

---

# âš½ Messi & Meaning â€” A Short Story

Messi touches the ball **less than others**.

Yet every touch **matters more**.

Common words are like defenders:
- they are everywhere
- they mean little

Rare words are like Messi:
- fewer appearances
- massive impact

That is **TF-IDF**.

---

# ğŸ† Python NLP â€” TF-IDF Edition (20 Problems)

Difficulty increases gradually.  
**Do not skip intuition.**

---

## ğŸŸ¢ TF-IDF 1 â€” Tokenization

```python
doc = "messi scores beautiful goals"
````

**Task:**
Convert sentence into tokens.

<details>
<summary>âœ… Solution</summary>

```python
tokens = doc.split()
print(tokens)
```

</details>

---

## ğŸŸ¢ TF-IDF 2 â€” Term Frequency (TF)

**TF(word) = count(word) / total words**

```python
doc = "messi scores goals goals"
```

<details>
<summary>âœ… Solution</summary>

```python
words = doc.split()
tf = {}

for w in words:
    tf[w] = tf.get(w, 0) + 1

for w in tf:
    tf[w] /= len(words)

print(tf)
```

</details>

---

## ğŸŸ¢ TF-IDF 3 â€” Vocabulary from Corpus

```python
docs = [
  "messi scores goals",
  "goals win matches",
  "messi inspires fans"
]
```

**Task:**
Extract unique vocabulary.

<details>
<summary>âœ… Solution</summary>

```python
vocab = set()
for d in docs:
    vocab |= set(d.split())
print(vocab)
```

</details>

---

## ğŸŸ¡ TF-IDF 4 â€” Document Frequency (DF)

**DF(word) = number of documents containing word**

<details>
<summary>âœ… Solution</summary>

```python
df = {}
for word in vocab:
    df[word] = sum(word in d.split() for d in docs)
print(df)
```

</details>

---

## ğŸŸ¡ TF-IDF 5 â€” Inverse Document Frequency (IDF)

**IDF(word) = log(N / DF)**

<details>
<summary>âœ… Solution</summary>

```python
import math

N = len(docs)
idf = {w: math.log(N / df[w]) for w in df}
print(idf)
```

</details>

---

## ğŸŸ¡ TF-IDF 6 â€” TF-IDF for One Document

<details>
<summary>âœ… Solution</summary>

```python
doc = docs[0]
words = doc.split()

tf = {}
for w in words:
    tf[w] = tf.get(w, 0) + 1
for w in tf:
    tf[w] /= len(words)

tfidf = {w: tf[w] * idf[w] for w in tf}
print(tfidf)
```

</details>

---

## ğŸ”µ TF-IDF 7 â€” Full TF-IDF Matrix

<details>
<summary>âœ… Solution</summary>

```python
matrix = []

for d in docs:
    words = d.split()
    tf = {}
    for w in words:
        tf[w] = tf.get(w, 0) + 1
    for w in tf:
        tf[w] /= len(words)

    vec = {w: tf.get(w, 0) * idf[w] for w in vocab}
    matrix.append(vec)

print(matrix)
```

</details>

---

## ğŸ”µ TF-IDF 8 â€” Important Words in Document

**Task:**
Find top-1 TF-IDF word.

<details>
<summary>âœ… Solution</summary>

```python
top = max(tfidf, key=tfidf.get)
print(top)
```

</details>

---

## ğŸ”µ TF-IDF 9 â€” Remove Stopwords

```python
stopwords = {"the","and","is","of"}
```

<details>
<summary>âœ… Solution</summary>

```python
filtered_docs = []
for d in docs:
    filtered_docs.append(
        " ".join(w for w in d.split() if w not in stopwords)
    )
print(filtered_docs)
```

</details>

---

## ğŸ”µ TF-IDF 10 â€” Normalize Vectors (Cosine Prep)

<details>
<summary>âœ… Solution</summary>

```python
import math

def norm(vec):
    return math.sqrt(sum(v*v for v in vec.values()))

print(norm(tfidf))
```

</details>

---

## ğŸ”´ TF-IDF 11 â€” Cosine Similarity

```python
a = "messi scores goals"
b = "messi inspires fans"
```

<details>
<summary>âœ… Solution</summary>

```python
def cosine(v1, v2):
    common = set(v1) & set(v2)
    num = sum(v1[w] * v2[w] for w in common)
    den = norm(v1) * norm(v2)
    return num / den if den else 0
```

</details>

---

## ğŸ”´ TF-IDF 12 â€” Find Most Similar Document

<details>
<summary>âœ… Solution</summary>

```python
query = matrix[0]
sims = [cosine(query, m) for m in matrix]
print(sims)
```

</details>

---

## ğŸ”´ TF-IDF 13 â€” Keyword Extraction

**Task:**
Return words with TF-IDF > threshold.

<details>
<summary>âœ… Solution</summary>

```python
keywords = [w for w,v in tfidf.items() if v > 0.2]
print(keywords)
```

</details>

---

## ğŸ”´ TF-IDF 14 â€” Streaming TF Update

<details>
<summary>âœ… Solution</summary>

```python
stream = ["messi scores", "scores again"]

tf = {}
total = 0

for s in stream:
    for w in s.split():
        tf[w] = tf.get(w, 0) + 1
        total += 1

for w in tf:
    tf[w] /= total

print(tf)
```

</details>

---

## ğŸ”´ TF-IDF 15 â€” Big Data MapReduce TF

<details>
<summary>âœ… Solution</summary>

```python
mapped = []
for d in docs:
    for w in d.split():
        mapped.append((w,1))

reduced = {}
for w,v in mapped:
    reduced[w] = reduced.get(w,0) + v

print(reduced)
```

</details>

---

## ğŸ”´ TF-IDF 16 â€” Rare Word Detection

**Task:**
Words with highest IDF.

<details>
<summary>âœ… Solution</summary>

```python
print(sorted(idf, key=idf.get, reverse=True))
```

</details>

---

## ğŸ”´ TF-IDF 17 â€” Explainability Test

**Question:**
Why does `"messi"` sometimes get low TF-IDF?

<details>
<summary>âœ… Answer</summary>

```text
Because it appears in many documents,
its IDF is low, reducing its importance.
```

</details>

---

## ğŸ”´ TF-IDF 18 â€” sklearn Comparison (Industry Standard)

<details>
<summary>âœ… Solution</summary>

```python
from sklearn.feature_extraction.text import TfidfVectorizer

vec = TfidfVectorizer()
X = vec.fit_transform(docs)

print(vec.get_feature_names_out())
print(X.toarray())
```

</details>

---

## ğŸ”´ TF-IDF 19 â€” Interview Question

**Why TF-IDF fails for semantics?**

<details>
<summary>âœ… Answer</summary>

```text
It ignores word order, context, and meaning.
Synonyms are treated as unrelated.
```

</details>

---

## ğŸ”´ TF-IDF 20 â€” Bridge to Transformers

**Question:**
What replaced TF-IDF?

<details>
<summary>âœ… Answer</summary>

```text
Word embeddings â†’ contextual embeddings â†’ transformers
```

</details>

---

## ğŸ¯ Final Message

TF-IDF teaches:

* importance
* rarity
* signal vs noise

If you understand this page:

* you understand **why attention exists**
* you understand **why embeddings work**

You are **ready for real NLP**.

---