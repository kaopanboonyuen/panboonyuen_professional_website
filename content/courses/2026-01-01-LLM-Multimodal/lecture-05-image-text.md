---
title: "Lecture 05 â€” Image â†” Text: Teaching Machines to See and Reason"
date: "2026-01-01"
type: book
weight: 50
---

<!--more-->

{{< icon name="clock" pack="fas" >}} ~5 hours (core multimodal lecture)

---

## ğŸŒ Why Imageâ€“Text Is So Important

Vision is the **dominant human sense**.

Images contain:
- spatial structure
- objects
- relationships
- context
- ambiguity

Teaching machines to **see and explain** is one of the hardest problems in AI.

> **Seeing is easy. Understanding is hard.**

---

## ğŸ§  What Is an Imageâ€“Text Multimodal System?

An **Imageâ€“Text-to-Text** system takes:
- ğŸ–¼ an image
- âœï¸ optional text (question, instruction)
- ğŸ§  performs reasoning
- ğŸ—£ produces text

Examples:
- Image captioning
- Visual Question Answering (VQA)
- Image-based reasoning
- Medical imaging reports
- Autonomous perception explanations

---

## ğŸ§© High-Level Architecture

```

Image
â†“
Vision Encoder (ViT / CNN)
â†“
Projection / Alignment
â†“
LLM (Reasoning)
â†“
Text Output

```

Key idea:
> **Vision models perceive. LLMs reason.**

---

## ğŸ§  Why Vision Alone Is Not Enough

Vision models are great at:
- detecting patterns
- recognizing objects
- learning spatial features

But they struggle with:
- logic
- explanation
- abstraction
- causality

> **Images do not reason. Language does.**

---

## ğŸ§ª Knowledge Check â€” Fundamentals

### Q1 (True / False)
A vision model alone can perform logical reasoning.

{{< spoiler text="Answer" >}}
False.
{{< /spoiler >}}

---

## ğŸ§  Core Imageâ€“Text Tasks

| Task | Input | Output |
|----|----|----|
| Image Captioning | Image | Text |
| VQA | Image + Question | Answer |
| Grounding | Image + Text | Region |
| Image QA | Image | Explanation |
| OCR + Reasoning | Image | Text + Answer |

This lecture focuses on **captioning + VQA**.

---

## ğŸ‘ Vision Encoders (The Eyes)

Popular encoders:
- ResNet (CNN-based)
- Vision Transformer (ViT)
- Swin Transformer
- ConvNeXt

They convert pixels â†’ embeddings.

---

## ğŸ§  Vision Encoder Output

```

Image â†’ Patch embeddings â†’ Sequence of vectors

```

Each patch represents **local visual semantics**.

---

## ğŸ§ª Knowledge Check â€” Vision

### Q2 (MCQ)
Which model introduced patch-based vision processing?

A) ResNet  
B) YOLO  
C) ViT  
D) AlexNet  

{{< spoiler text="Correct Answer" >}}
C) ViT
{{< /spoiler >}}

---

## ğŸ§  The Alignment Problem (CRITICAL)

Vision embeddings â‰  language embeddings.

Alignment answers:
> **How does a pixel become a word?**

---

## ğŸ§© Alignment Strategies

| Strategy | Description |
|----|----|
| Linear projection | Simple, efficient |
| MLP | Nonlinear mapping |
| Cross-attention | Deep fusion |
| Q-Former | Learned query alignment |

> **Most failures come from poor alignment.**

---

## ğŸ§ª Knowledge Check â€” Alignment

### Q3 (Objective)
What is the purpose of the projection layer?

{{< spoiler text="Answer" >}}
To map vision embeddings into the language embedding space.
{{< /spoiler >}}

---

## ğŸ§  CLIP â€” A Foundational Breakthrough

CLIP learned:
```

Image â†” Text similarity

````

By contrastive learning:
- match image with correct caption
- separate mismatched pairs

Result:
> **Shared semantic space**

---

## ğŸ§ª Knowledge Check â€” CLIP

### Q4 (MCQ)
What learning paradigm does CLIP use?

A) Supervised classification  
B) Reinforcement learning  
C) Contrastive learning  
D) Autoencoding  

{{< spoiler text="Correct Answer" >}}
C) Contrastive learning
{{< /spoiler >}}

---

## ğŸ Python Lab 1 â€” Image Captioning with BLIP

### ğŸ“¦ Install Dependencies

```bash
pip install transformers pillow torch torchvision
````

---

### ğŸ§  Load Model

```python
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image

model_name = "Salesforce/blip-image-captioning-base"

processor = BlipProcessor.from_pretrained(model_name)
model = BlipForConditionalGeneration.from_pretrained(model_name)
```

---

### ğŸ–¼ Load Image

```python
image = Image.open("example.jpg").convert("RGB")
```

---

### âœï¸ Generate Caption

```python
inputs = processor(image, return_tensors="pt")

out = model.generate(**inputs, max_new_tokens=50)

caption = processor.decode(out[0], skip_special_tokens=True)
print(caption)
```

> ğŸ‰ You just taught a machine to describe what it sees.

---

## ğŸ§ª Knowledge Check â€” Code

### Q5 (Objective)

What role does the processor play in BLIP?

{{< spoiler text="Answer" >}}
It preprocesses images and decodes generated tokens.
{{< /spoiler >}}

---

## ğŸ§  Adding Reasoning with an LLM

Captioning â‰  understanding.

We add an LLM to:

* answer questions
* infer relationships
* explain scenes

---

## ğŸ— Full Imageâ€“Text Reasoning Pipeline

```
Image â†’ Vision Encoder â†’ Alignment â†’ LLM â†’ Answer
```

Optional:

```
+ Question
```

---

## ğŸ Python Lab 2 â€” Visual Question Answering

```python
question = "What is the person doing in the image?"

prompt = f"""
You are a vision-language assistant.
Image description:
{caption}

Question:
{question}
"""

from transformers import AutoTokenizer, AutoModelForCausalLM

llm_name = "meta-llama/Llama-3-8B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(llm_name)
llm = AutoModelForCausalLM.from_pretrained(llm_name)

inputs = tokenizer(prompt, return_tensors="pt")
outputs = llm.generate(**inputs, max_new_tokens=100)

answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(answer)
```

---

## ğŸ§ª Knowledge Check â€” Reasoning

### Q6 (True / False)

Image captioning alone is sufficient for VQA.

{{< spoiler text="Answer" >}}
False.
{{< /spoiler >}}

---

## ğŸ§  Common Failure Modes

* Hallucinated objects
* Missed small details
* Incorrect spatial relations
* Cultural bias
* Overconfidence

> **Seeing â‰  understanding â‰  truth**

---

## ğŸ§ª Knowledge Check â€” Failures

### Q7 (MCQ)

Which failure is most dangerous?

A) Slow inference
B) Hallucinated objects
C) Low resolution
D) Long captions

{{< spoiler text="Correct Answer" >}}
B) Hallucinated objects
{{< /spoiler >}}

---

## ğŸ§  Evaluation Metrics

| Metric       | Use                    |
| ------------ | ---------------------- |
| BLEU / CIDEr | Caption quality        |
| VQA Accuracy | QA correctness         |
| Human Eval   | Trust & clarity        |
| Calibration  | Confidence reliability |

---

## ğŸ§ª Knowledge Check â€” Evaluation

### Q8 (Objective)

Why is human evaluation important for vision-language tasks?

{{< spoiler text="Answer" >}}
Because correctness depends on semantics and human perception.
{{< /spoiler >}}

---

## ğŸŒ± Ethics in Visionâ€“Language AI

Risks:

* surveillance
* facial recognition abuse
* bias in datasets
* privacy violation

> **If a machine sees people, it must respect humanity.**

---

## ğŸ§ª Knowledge Check â€” Ethics

### Q9 (True / False)

Imageâ€“text systems can be ethically neutral.

{{< spoiler text="Answer" >}}
False.
{{< /spoiler >}}

---

## ğŸ§  Human-in-the-Loop (Best Practice)

* Human review for sensitive outputs
* Confidence thresholds
* Explainable responses
* Feedback-based refinement

---

## âœ… Final Takeaways

* Vision provides perception
* Language provides reasoning
* Alignment is the hardest part
* Python pipelines are modular
* Ethics is not optional

---

## ğŸŒ Final Reflection

{{< spoiler text="If a machine describes a human incorrectly, who is harmed?" >}}
The human â€” therefore responsibility lies with designers.
{{< /spoiler >}}

---